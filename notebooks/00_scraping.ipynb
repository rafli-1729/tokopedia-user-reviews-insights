{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7130ebb",
   "metadata": {},
   "source": [
    "# üëì **Overview**\n",
    "This notebook documents the process of collecting user reviews from the Tokopedia Android application using the `google-play-scraper` Python library. The resulting dataset will be used for downstream tasks such as text cleaning, sentiment analysis, and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ceaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet google-play-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03aa10",
   "metadata": {},
   "source": [
    "# üõª **Data Source**\n",
    "The dataset is collected from the Google Play Store using the `google-play-scraper` library, targeting the Tokopedia application `com.tokopedia.tkpd`. All reviews originate from Indonesian users, providing domain-specific language patterns relevant to the local market. Each entry includes the review text, rating score, and review date. Since Google Play only exposes one review per user for a given app, we do not need a separate review ID, as duplication at the user level is inherently prevented by the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f750b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Core library\n",
    "import pandas as pd\n",
    "\n",
    "# Web scraping\n",
    "from google_play_scraper import (\n",
    "    Sort,\n",
    "    reviews\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a6bce",
   "metadata": {},
   "source": [
    "# ‚õèÔ∏è **Scraping Method**\n",
    "We use the `reviews()` function from `google-play-scraper`, which provides a high-level interface for retrieving app reviews without the need to manually parse HTML or handle pagination. We'll fetches reviews in chunks of 100.000 using the continuation token returned by `reviews()`. All retrieved entries are stored in a list, which will later be converted into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b349e423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 100000 data\n",
      "Successfully scraped 200000 data\n",
      "Successfully scraped 300000 data\n",
      "Successfully scraped 400000 data\n",
      "Successfully scraped 500000 data\n",
      "Successfully scraped 600000 data\n",
      "Successfully scraped 700000 data\n",
      "Successfully scraped 800000 data\n",
      "Successfully scraped 900000 data\n",
      "Successfully scraped 1000000 data\n",
      "\n",
      "Data scraped successfully!\n"
     ]
    }
   ],
   "source": [
    "all_reviews = []\n",
    "continuation_token = None\n",
    "\n",
    "for _ in range(10):\n",
    "    batch, continuation_token = reviews(\n",
    "        'com.tokopedia.tkpd',\n",
    "        lang='id',\n",
    "        country='id',\n",
    "        sort=Sort.NEWEST,\n",
    "        count=100000,\n",
    "        continuation_token=continuation_token\n",
    "    )\n",
    "\n",
    "    all_reviews.extend(batch)\n",
    "    if continuation_token is None:\n",
    "        break\n",
    "\n",
    "    print(f'Successfully scraped {(_+1)*100000} data')\n",
    "\n",
    "print('\\nData scraped successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>char_len</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 11:01:10</td>\n",
       "      <td>87.0</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Memuaskan kan produk original</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:35:41</td>\n",
       "      <td>29.0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 10:11:21</td>\n",
       "      <td>124.0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jos mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:04:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 09:54:44</td>\n",
       "      <td>66.0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "0                                       belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai   \n",
       "1                                                                                                 Memuaskan kan produk original   \n",
       "2  mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya   \n",
       "3                                                                                                                    jos mantap   \n",
       "4                                                            Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK   \n",
       "\n",
       "   rating                 date  char_len  token_len  \n",
       "0       5  2025-12-03 11:01:10      87.0        127  \n",
       "1       5  2025-12-03 10:35:41      29.0         42  \n",
       "2       1  2025-12-03 10:11:21     124.0        185  \n",
       "3       5  2025-12-03 10:04:00      10.0         17  \n",
       "4       1  2025-12-03 09:54:44      66.0        103  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scrape = pd.DataFrame(all_reviews)\n",
    "\n",
    "df_raw = df_scrape[['content', 'score', 'at']]\n",
    "df_raw.columns = ['text', 'rating', 'date']\n",
    "\n",
    "# Additional features for further exploration\n",
    "df_raw[\"char_len\"] = df_raw[\"text\"].str.len()\n",
    "df_raw[\"token_len\"] = (\n",
    "    df_raw[\"text\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.split()\n",
    "    .apply(len)\n",
    ")\n",
    "\n",
    "print(df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4fbd1",
   "metadata": {},
   "source": [
    "Even though we scraped to 1.000.000 review, library `google-play-scraper` only returning 709.000 of reviews range from year 2020 to 2025. There is no problem since this amount of data is already large for our projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8653666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data successfully saved to \"../data/raw/review.csv\"\n"
     ]
    }
   ],
   "source": [
    "df_raw.to_csv('../data/raw/review.csv', index=False)\n",
    "\n",
    "print('Scraped data successfully saved to \"../data/raw/review.csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a11892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped review only successfully saved to \"../data/raw/all_reviews.txt\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/raw/all_reviews.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df_raw.text.astype(str):\n",
    "        f.write(line.replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "print('Scraped review only successfully saved to \"../data/raw/all_reviews.txt\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094cfec",
   "metadata": {},
   "source": [
    "# üöß **Limitations**\n",
    "Although the scraper is designed for large-scale data collection, Google Play imposes restrictions that prevent retrieving the full set of available reviews. As a result, the scraping process may stop earlier than expected, yielding fewer reviews than the theoretical total. These limitations stem from API rate controls, pagination boundaries, and Google‚Äôs internal filtering, all of which can cap the maximum number of accessible reviews regardless of how many exist on the platform.\n",
    "  \n",
    "# ü™® **Next Steps**\n",
    "The next step is to clean and normalize the raw review text, as the dataset is still highly noisy and contains various artifacts such as emojis, repeated characters, inconsistent slang, typos, and formatting irregularities. Preparing the text through structured cleaning will ensure that downstream analysis and modeling operate on stable, standardized input rather than raw unprocessed data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee481b7",
   "metadata": {},
   "source": [
    "# üßº Text Cleaning for Tokopedia User Reviews  \n",
    "This notebook performs systematic text cleaning on raw, user-generated reviews collected from the Tokopedia application.\n",
    "\n",
    "User reviews typically contain substantial noise, such as:\n",
    "\n",
    "- emojis and unicode icons\n",
    "- URLs and emails\n",
    "- excessive character repetitions (‚Äúbaaaagus bangeeetttt‚Äù)\n",
    "- exaggerated laughter (‚Äúwkwkwkwkwk‚Äù, ‚Äúhahahahaha‚Äù)\n",
    "- slang and informal spellings (‚Äúgk‚Äù, ‚Äúga‚Äù, ‚Äúbgt‚Äù, ‚Äúplis‚Äù)\n",
    "- typos and phonetic spelling\n",
    "- punctuation noise\n",
    "- extremely short or low-information messages (‚Äúok‚Äù, ‚Äú.‚Äù)\n",
    "\n",
    "Cleaning these reviews is essential to:\n",
    "\n",
    "- reduce vocabulary sparsity  \n",
    "- standardize spelling variations  \n",
    "- improve downstream NLP model quality  \n",
    "- remove meaningless tokens  \n",
    "- prepare the text for vectorization and modeling  \n",
    "\n",
    "This notebook runs through the process **step-by-step**, showing before/after transformations to highlight the effect of each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df6e9f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Directory alignment and module update\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Ignore warning\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Core library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cleaning tools\n",
    "import re\n",
    "import src.cleaning as cleaning\n",
    "\n",
    "# Reload shortcut\n",
    "def r(module=cleaning):\n",
    "    importlib.reload(module)\n",
    "\n",
    "# Defaults\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643a677",
   "metadata": {},
   "source": [
    "# üîç Load Raw Review Data  \n",
    "\n",
    "We start by loading the unprocessed user reviews from the dataset. Only the raw text column will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a31dfa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aplikasi bagun untuk belanja</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-26 16:54:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sudah mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-26 16:52:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tokopedia sekarang jadi ribet tidak seperti dulu lagi kalo mau return barang yang tidak sesuai harus nunggu waktu terlalu lama jadi males belanja lagi di tokopedia saya auto unistal, tidak seperti shopee yang mudah dan enak dan sekarang belaja terus di shopee</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-11-26 16:30:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kasih bintang 1 ,karena ngisi kouta saja lama prosesnya</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-11-26 15:05:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>di janjikan dapet kompensasi atas keterlambatan pengiriman sameday yg gak sesuai estimasi, sampai sekarang udh 8 hari kerja blm dapet jga gimana sih Tokopedia</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-11-26 15:03:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                              raw_text  \\\n",
       "0                                                                                                                                                                                                                                         Aplikasi bagun untuk belanja   \n",
       "1                                                                                                                                                                                                                                                         sudah mantap   \n",
       "2  tokopedia sekarang jadi ribet tidak seperti dulu lagi kalo mau return barang yang tidak sesuai harus nunggu waktu terlalu lama jadi males belanja lagi di tokopedia saya auto unistal, tidak seperti shopee yang mudah dan enak dan sekarang belaja terus di shopee   \n",
       "3                                                                                                                                                                                                              kasih bintang 1 ,karena ngisi kouta saja lama prosesnya   \n",
       "4                                                                                                       di janjikan dapet kompensasi atas keterlambatan pengiriman sameday yg gak sesuai estimasi, sampai sekarang udh 8 hari kerja blm dapet jga gimana sih Tokopedia   \n",
       "\n",
       "   rating                 date  \n",
       "0       4  2025-11-26 16:54:58  \n",
       "1       5  2025-11-26 16:52:43  \n",
       "2       1  2025-11-26 16:30:31  \n",
       "3       1  2025-11-26 15:05:33  \n",
       "4       1  2025-11-26 15:03:32  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/review.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868cda1",
   "metadata": {},
   "source": [
    "# üìö Load Cleaning Resources  \n",
    "\n",
    "The cleaning pipeline uses several external resources stored in `resources/`:\n",
    "\n",
    "- **slang.json** ‚Äî a mapping from slang words to their normalized forms  \n",
    "- **stopwords_extra.txt** ‚Äî additional informal stopwords not found in standard lists  \n",
    "- **fuzzy_targets.json** ‚Äî canonical words frequently affected by typos or misspellings  \n",
    "\n",
    "These resources supplement the cleaning functions defined in `src/cleaning.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f2d443ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/slang.json\") as f:\n",
    "    slang = json.load(f)\n",
    "\n",
    "with open(\"../resources/stopwords.txt\") as f:\n",
    "    stopwords = [x.strip() for x in f]\n",
    "\n",
    "with open(\"../resources/fuzzy_targets.json\") as f:\n",
    "    fuzzy_targets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b382d",
   "metadata": {},
   "source": [
    "# üìù Example Raw Review  \n",
    "\n",
    "Let‚Äôs inspect the most noisy raw review to understand the noise present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84221938",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò° cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,, lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed34f82",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Lowercasing & Removing URLs/Emails  \n",
    "\n",
    "User reviews often contain URLs, emails, or random capitalizations. These introduce unnecessary variance into the vocabulary and should be standardized early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0cea687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò° cek ini deh:   gk tauuu kenapaaa, email-ku:   lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = example.lower()\n",
    "step1 = re.sub(r\"http\\S+|www\\.\\S+|\\S+@\\S+\", \" \", step1)\n",
    "step1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac02c52",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Remove Emoji  \n",
    "\n",
    "Emojis add noise to tokenization and typically do not contribute meaningful information for text modeling. We remove them using a Unicode-based pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fd1fdc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wkwkwkwkwk gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh  cek ini deh:   gk tauuu kenapaaa, email-ku:   lamaaaaaa bangettttt prosesnyyyyaaaa  sm tolongggg bgt dongggg!!!!'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = cleaning.remove_emoji(step1)\n",
    "step2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6730c",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Normalize Laughter Patterns  \n",
    "\n",
    "Indonesian users frequently express laughter using patterns such as:  \n",
    "\n",
    "- ‚Äúwkwkwkwk‚Äù\n",
    "- ‚Äúwkwwkkwkw‚Äù\n",
    "- ‚Äúhahahahaha‚Äù\n",
    "\n",
    "We normalize these exaggerated sequences into a canonical form (‚Äúwkwk‚Äù, ‚Äúhaha‚Äù) to reduce vocabulary explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f230bc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wkwk gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh  cek ini deh:   gk tauuu kenapaaa, email-ku:   lamaaaaaa bangettttt prosesnyyyyaaaa  sm tolongggg bgt dongggg!!!!'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = cleaning.normalize_laughter(step2)\n",
    "step3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80d1cc",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Collapse Repeated Characters  \n",
    "\n",
    "Over-emphasized expressions such as ‚Äúbaaaagusss‚Äù or ‚Äúbangeeettt‚Äù introduce many unique tokens. We collapse any character repeated more than twice into a single instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "254ee508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wkwk gk bisa login skrg pls help! lmoott bnaget. sumpah  cek ini deh: gk tau kenapa, email-ku: lama banget prosesnya  sm tolong bgt dong!'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step4 = cleaning.collapse_repeated_chars(step3)\n",
    "step4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e3006",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Normalize Vowel Stretching  \n",
    "\n",
    "In informal Indonesian text, users often elongate vowels to express emotion (‚Äúlaaaamaaa‚Äù). We reduce these to their canonical vowel forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7aa48b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wkwk gk bisa login skrg pls help! lmott bnaget. sumpah  cek ini deh: gk tau kenapa, email-ku: lama banget prosesnya  sm tolong bgt dong!'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step5 = cleaning.normalize_vowel_stretch(step4)\n",
    "step5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24839c57",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Remove Punctuation \n",
    "\n",
    "Removing punctuation before slang/fuzzy lookup ensures tokens match dictionary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3189df26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wkwk gk bisa login skrg pls help  lmott bnaget  sumpah  cek ini deh  gk tau kenapa  email ku  lama banget prosesnya  sm tolong bgt dong '"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step6 = cleaning.remove_punctuation(step5)\n",
    "step6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee422e43",
   "metadata": {},
   "source": [
    "## Step 7 ‚Äî Slang Normalization  \n",
    "\n",
    "Slang expressions like:  \n",
    "- ‚Äúgk‚Äù  \n",
    "- ‚Äúga‚Äù  \n",
    "- ‚Äúsm‚Äù  \n",
    "- ‚Äúbgt‚Äù  \n",
    "\n",
    "are replaced using a predefined slang dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "92e40ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tertawa tidak bisa login sekarang tolong help lmott bnaget sumpah cek ini deh tidak tahu kenapa email ku lama banget prosesnya sama tolong banget dong'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step7 = cleaning.normalize_slang(step6, slang)\n",
    "step7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d126fd",
   "metadata": {},
   "source": [
    "## Step 8 ‚Äî Fuzzy Normalization  \n",
    "\n",
    "Typographical variations such as:\n",
    "- ‚Äúbangett‚Äù\n",
    "- ‚Äúbnaget‚Äù\n",
    "- ‚Äúbangeet‚Äù\n",
    "- ‚Äúlemott‚Äù\n",
    "\n",
    "are mapped back into canonical forms (‚Äúbanget‚Äù, ‚Äúlemot‚Äù) using fuzzy similarity scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d0d13454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tertawa tidak bisa login sekarang tolong help lemot banget sumpah cek ini deh tidak tahu kenapa email ku lama banget prosesnya sama tolong banget dong'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step8 = cleaning.fuzzy_normalize(step7, fuzzy_targets)\n",
    "step8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a6f2e",
   "metadata": {},
   "source": [
    "## Step 9 ‚Äî Remove Stopwords  \n",
    "\n",
    "We remove additional informal stopwords (e.g., ‚Äúsih‚Äù, ‚Äúdong‚Äù, ‚Äúlah‚Äù) to focus on content-bearing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "38579421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tertawa login tolong help lemot banget sumpah cek deh email ku banget prosesnya tolong banget'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step9 = cleaning.remove_stopwords(step8, stopwords)\n",
    "step9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33d283",
   "metadata": {},
   "source": [
    "## Step 10 ‚Äî Remove Low-Information Reviews  \n",
    "\n",
    "Extremely short or non-informative texts (e.g., ‚Äúok‚Äù, ‚Äú.‚Äù) are dropped entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e0a56cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tertawa login tolong help lemot banget sumpah cek deh email ku banget prosesnya tolong banget'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_example = cleaning.drop_lowinfo(step9)\n",
    "final_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4487e",
   "metadata": {},
   "source": [
    "# üßπ After Cleaning Review\n",
    "\n",
    "After all the cleaning pipeline, we'll end up with more valueable review text as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4dffc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "849705b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò° cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,, lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!</td>\n",
       "      <td>tertawa login tolong help lemot banget sumpah cek deh email ku banget prosesnya tolong banget</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                         raw_text  \\\n",
       "0  WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò° cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,, lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!   \n",
       "\n",
       "                                                                                    cleaned_text  \n",
       "0  tertawa login tolong help lemot banget sumpah cek deh email ku banget prosesnya tolong banget  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"raw_text\": [example],\n",
    "    \"cleaned_text\": [final_example]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3a4a8",
   "metadata": {},
   "source": [
    "# üöÄ Applying the Full Cleaning Pipeline\n",
    "\n",
    "Now that each cleaning step has been validated individually,\n",
    "we apply the full `clean_text()` function to the entire dataset.\n",
    "\n",
    "This ensures all reviews follow a standardized, noise-free text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072b8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff05b06b05de464e8b621980545d5887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df[\"clean_text\"] = df[\"raw_text\"].progress_apply(\n",
    "    lambda x: cleaning.clean_text(\n",
    "        x,\n",
    "        slang=slang,\n",
    "        stopwords=stopwords,\n",
    "        fuzzy_targets=fuzzy_targets\n",
    "    )\n",
    ")\n",
    "\n",
    "print('finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee481b7",
   "metadata": {},
   "source": [
    "# üßº **Text Cleaning for Tokopedia User Reviews**  \n",
    "This notebook performs systematic text cleaning on raw, user-generated reviews collected from the Tokopedia application.\n",
    "\n",
    "User reviews typically contain substantial noise, such as:\n",
    "\n",
    "- emojis and unicode icons\n",
    "- URLs and emails\n",
    "- excessive character repetitions (‚Äúbaaaagus bangeeetttt‚Äù)\n",
    "- exaggerated laughter (‚Äúwkwkwkwkwk‚Äù, ‚Äúhahahahaha‚Äù)\n",
    "- slang and informal spellings (‚Äúgk‚Äù, ‚Äúga‚Äù, ‚Äúbgt‚Äù, ‚Äúplis‚Äù)\n",
    "- typos and phonetic spelling\n",
    "- punctuation noise\n",
    "- extremely short or low-information messages (‚Äúok‚Äù, ‚Äú.‚Äù)\n",
    "\n",
    "Cleaning these reviews is essential to:\n",
    "\n",
    "- reduce vocabulary sparsity  \n",
    "- standardize spelling variations  \n",
    "- improve downstream NLP model quality  \n",
    "- remove meaningless tokens  \n",
    "- prepare the text for vectorization and modeling  \n",
    "\n",
    "This notebook runs through the process **step-by-step**, showing before/after transformations to highlight the effect of each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6e9f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Directory alignment and module update\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Ignore warning\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Core library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cleaning tools\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import src.cleaning as cleaning\n",
    "from src.cleaning import *\n",
    "from src.cleaning import CleaningPipeline\n",
    "\n",
    "# Reload shortcut\n",
    "def r(module=cleaning):\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Defaults\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643a677",
   "metadata": {},
   "source": [
    "# üîç **Load Raw Review Data**  \n",
    "\n",
    "We start by loading the unprocessed user reviews from the dataset. Only the raw text column will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31dfa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 11:01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Memuaskan kan produk original</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:35:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 10:11:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jos mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 09:54:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "0                                       belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai   \n",
       "1                                                                                                 Memuaskan kan produk original   \n",
       "2  mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya   \n",
       "3                                                                                                                    jos mantap   \n",
       "4                                                            Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK   \n",
       "\n",
       "   rating                 date  \n",
       "0       5  2025-12-03 11:01:10  \n",
       "1       5  2025-12-03 10:35:41  \n",
       "2       1  2025-12-03 10:11:21  \n",
       "3       5  2025-12-03 10:04:00  \n",
       "4       1  2025-12-03 09:54:44  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/review.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868cda1",
   "metadata": {},
   "source": [
    "# üìö **Load Cleaning Resources**  \n",
    "\n",
    "The cleaning pipeline uses several external resources stored in `resources/`:\n",
    "\n",
    "- **slang.json**: a mapping from slang words to their normalized forms.  \n",
    "- **stopwords.txt**: additional informal stopwords not found in standard lists  \n",
    "- **whitelist.txt**: ground truth of indonesian word based on KBBI (Kamus Besar Bahasa Indoensia)\n",
    "- **fuzzy_targets.json**: canonical words frequently affected by typos or misspellings \n",
    "\n",
    "These resources supplement the cleaning functions defined in `src/cleaning.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2d443ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/emoji_map.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    emoji = json.load(f)\n",
    "\n",
    "with open(\"../assets/pos_lexicon.json\") as f:\n",
    "    pos_lexicon = json.load(f)\n",
    "\n",
    "with open(\"../assets/prefix_suffix.json\") as f:\n",
    "    prefix_suffix = json.load(f)\n",
    "\n",
    "with open(\"../assets/slang.json\") as f:\n",
    "    slang = json.load(f)\n",
    "\n",
    "with open(\"../assets/typo.json\") as f:\n",
    "    typo = json.load(f)\n",
    "\n",
    "with open(\"../assets/affix_map.json\") as f:\n",
    "    affix_map = json.load(f)\n",
    "\n",
    "with open(\"../assets/stopwords.txt\") as f:\n",
    "    stopwords = [x.strip() for x in f]\n",
    "\n",
    "with open(\"../assets/whitelist.txt\") as f:\n",
    "    whitelist = [x.strip() for x in f]\n",
    "\n",
    "with open(\"../assets/laughter.txt\") as f:\n",
    "    laughter = [x.strip() for x in f]\n",
    "\n",
    "with open(\"../assets/negation.txt\") as f:\n",
    "    negation = [x.strip() for x in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e54b5a",
   "metadata": {},
   "source": [
    "# **Baseline Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94c27f",
   "metadata": {},
   "source": [
    "Lowercase, remove link, remove email, remove emoji, remove punctuation, unicode cleaning. To make this process more efficient, we'll make a dictionary of cached words that has been normalized before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6697cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r(cleaning)\n",
    "\n",
    "cleaner = cleaning.CleaningPipeline(\n",
    "    whitelist=whitelist, slang=slang, typo=typo, prefix_suffix=prefix_suffix,\n",
    "    emoji_map=emoji, laughter_list=laughter, stopwords=stopwords,\n",
    "    pos_lexicon=pos_lexicon, negation_list=negation, affix_map=affix_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b72bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4f671946a94fea974eeab599e03e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STRETCH_CACHE = {}\n",
    "LAUGHTER_CACHE = {}\n",
    "\n",
    "def cached_stretch(word):\n",
    "    w = word.lower()\n",
    "\n",
    "    if w in STRETCH_CACHE:\n",
    "        return STRETCH_CACHE[w]\n",
    "\n",
    "    result = cleaner._stretch_all(w)\n",
    "\n",
    "    STRETCH_CACHE[w] = result\n",
    "    return result\n",
    "\n",
    "def cached_laughter(word):\n",
    "    w = word.lower()\n",
    "\n",
    "    if w in LAUGHTER_CACHE:\n",
    "        return LAUGHTER_CACHE[w]\n",
    "\n",
    "    result = cleaner._normalize_laughter(w)\n",
    "\n",
    "    LAUGHTER_CACHE[w] = result\n",
    "    return result\n",
    "\n",
    "# Baseline cleaning\n",
    "def baseline_cleaning(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = cleaner._normalize_unicode(text)\n",
    "    text = text.lower()\n",
    "    text = cleaner._remove_email_and_link(text)\n",
    "    text = cleaner._remove_punctuation(text)\n",
    "    text = cleaner._handle_word_number(text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [\n",
    "        cached_stretch(tok)\n",
    "        for tok in tokens\n",
    "    ]\n",
    "    tokens = [\n",
    "        cached_laughter(tok)\n",
    "        for tok in tokens\n",
    "    ]\n",
    "    text = \" \".join(tokens)\n",
    "    text = cleaner._map_emoji(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df[\"text_\"] = df[\"text\"].progress_apply(\n",
    "    lambda sentence: baseline_cleaning(sentence)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bccfb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a737de1aff34eb0a7efccaf76a1dbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['_text_'] = df['text'].progress_apply(\n",
    "    lambda sentence: cleaner.explain(sentence, verbose=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709744f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>text_</th>\n",
       "      <th>_text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 11:01:10</td>\n",
       "      <td>belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai</td>\n",
       "      <td>belanja sangat mudah sayang estimasi pengiriman tidak sesuai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Memuaskan kan produk original</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:35:41</td>\n",
       "      <td>memuaskan kan produk original</td>\n",
       "      <td>memuaskan produk original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 10:11:21</td>\n",
       "      <td>mau nyari apa aja di mesin pencarianya tokopedia hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya</td>\n",
       "      <td>mencari mesin pencarianya hasil timeout melulu padahal jaringan bagus main game online lancar jaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jos mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:04:00</td>\n",
       "      <td>jos mantap</td>\n",
       "      <td>jos mantap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 09:54:44</td>\n",
       "      <td>tidak punya cs hanya ada bot yg tidak bisa memberikan solusi buruk</td>\n",
       "      <td>tidak customer service bot tidak bisa memberikan solusi buruk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "0                                       belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai   \n",
       "1                                                                                                 Memuaskan kan produk original   \n",
       "2  mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya   \n",
       "3                                                                                                                    jos mantap   \n",
       "4                                                            Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK   \n",
       "\n",
       "   rating                 date  \\\n",
       "0       5  2025-12-03 11:01:10   \n",
       "1       5  2025-12-03 10:35:41   \n",
       "2       1  2025-12-03 10:11:21   \n",
       "3       5  2025-12-03 10:04:00   \n",
       "4       1  2025-12-03 09:54:44   \n",
       "\n",
       "                                                                                                                         text_  \\\n",
       "0                                      belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai   \n",
       "1                                                                                                memuaskan kan produk original   \n",
       "2  mau nyari apa aja di mesin pencarianya tokopedia hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya   \n",
       "3                                                                                                                   jos mantap   \n",
       "4                                                           tidak punya cs hanya ada bot yg tidak bisa memberikan solusi buruk   \n",
       "\n",
       "                                                                                               _text_  \n",
       "0                                        belanja sangat mudah sayang estimasi pengiriman tidak sesuai  \n",
       "1                                                                           memuaskan produk original  \n",
       "2  mencari mesin pencarianya hasil timeout melulu padahal jaringan bagus main game online lancar jaya  \n",
       "3                                                                                          jos mantap  \n",
       "4                                       tidak customer service bot tidak bisa memberikan solusi buruk  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3475a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Cleaning\n",
    "df_interim = df[['text_', 'rating', 'date']]\n",
    "df_interim.columns = ['text', 'rating', 'date']\n",
    "df_interim.to_csv('../data/interim/review.csv', index=False)\n",
    "\n",
    "df_interim['text'] = df_interim['text'].astype(str).str.strip()\n",
    "df_interim = df_interim[df_interim['text'] != \"\"]\n",
    "\n",
    "# Full Cleaning\n",
    "df_clean = df[['_text_', 'rating', 'date']]\n",
    "df_clean.columns = ['text', 'rating', 'date']\n",
    "df_clean.to_csv('../data/processed/review.csv', index=False)\n",
    "\n",
    "df_clean['text'] = df_clean['text'].astype(str).str.strip()\n",
    "df_clean = df_clean[df_clean['text'] != \"\"]\n",
    "\n",
    "with open(\"../data/interim/all_reviews.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df_interim.text.astype(str):\n",
    "        f.write(line.replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "with open(\"../data/processed/all_reviews.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df_clean.text.astype(str):\n",
    "        f.write(line.replace(\"\\n\", \" \") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6820c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 23748/28559 [28:28<06:42, 11.94it/s] "
     ]
    }
   ],
   "source": [
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 1. Sastrawi Stemmer\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "# affix_map = {root: set() for root in whitelist}\n",
    "\n",
    "# for tok in tqdm(set(tokens_interim), desc=\"Processing tokens\"):\n",
    "#     tok = tok.strip()\n",
    "#     if not tok:\n",
    "#         continue\n",
    "\n",
    "#     stem = stemmer.stem(tok)\n",
    "\n",
    "#     if stem in whitelist:\n",
    "#         affix_map[stem].add(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318abecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_root = {}\n",
    "\n",
    "# for root, forms in affix_map.items():\n",
    "\n",
    "#     clean_root = baseline_cleaning(root).strip()\n",
    "#     if not clean_root:\n",
    "#         continue\n",
    "\n",
    "#     for word in forms:\n",
    "#         clean_word = baseline_cleaning(word).strip()\n",
    "\n",
    "#         if not clean_word:\n",
    "#             continue\n",
    "\n",
    "#         # skip kata dasar ‚Üí kata dasar (aba:aba, abad:abad)\n",
    "#         if clean_word == clean_root:\n",
    "#             continue\n",
    "\n",
    "#         # skip kalau lebih dari 1 kata\n",
    "#         if \" \" in clean_word:\n",
    "#             continue\n",
    "\n",
    "#         token_to_root[clean_word] = clean_root\n",
    "\n",
    "# with open(\"../assets/affix_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(token_to_root, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747cf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitelist original: 28494\n",
      "Whitelist filtered: 2304\n"
     ]
    }
   ],
   "source": [
    "# unique_tokens_clean = set(tokens_interim)\n",
    "\n",
    "# # ambil stems unik\n",
    "# unique_stems = set(token_to_root.values())\n",
    "\n",
    "# # FILTER whitelist\n",
    "# whitelist_filtered = unique_stems.intersection(whitelist)\n",
    "\n",
    "# print(\"Whitelist original:\", len(whitelist))\n",
    "# print(\"Whitelist filtered:\", len(whitelist_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50c14d",
   "metadata": {},
   "source": [
    "# üñãÔ∏è Possible Typo Mappings\n",
    "\n",
    "With our resources above, we can make a list of typo hypothetically based on word that did not appear on our whitelist or slang dictionary. This process can help us to detect any possible typo and add them into our external resources to make dataset even more clean.\n",
    "\n",
    "In order to extract unique tokens from our dataset, we'll normalize them to lowercase, removing punctuation, stripping emojis, split digit word, collapse whitespaces, normalize unicode, and normalize laughter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4496db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 486539 entries, 0 to 708999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    486539 non-null  object\n",
      " 1   rating  486539 non-null  int64 \n",
      " 2   date    486539 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 14.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84210856",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_raw = [\n",
    "    word\n",
    "    for sentence in df.text.astype(str)\n",
    "    for word in sentence.split()\n",
    "]\n",
    "\n",
    "tokens_interim = [\n",
    "    word\n",
    "    for sentence in df_interim.text.astype(str)\n",
    "    for word in sentence.split()\n",
    "]\n",
    "\n",
    "tokens_clean = [\n",
    "    word\n",
    "    for sentence in df_clean.text.astype(str)\n",
    "    for word in sentence.split()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86f87e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tokens length         :  5412151\n",
      "Interim Tokens Length     :  5587881\n",
      "Fully Clean Tokens Length :  4035205\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Tokens length         : \", len(tokens_raw))\n",
    "print(\"Interim Tokens Length     : \", len(tokens_interim))\n",
    "print(\"Fully Clean Tokens Length : \", len(tokens_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9416e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw uniques tokens length     :  303552\n",
      "Interim uniques tokens length :  77738\n",
      "Clean uniques tokens length   :  59804\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw uniques tokens length     : \", len(set(tokens_raw)))\n",
    "print(\"Interim uniques tokens length : \", len(set(tokens_interim)))\n",
    "print(\"Clean uniques tokens length   : \", len(set(tokens_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ae16174",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_interim_set = set(tokens_clean)       # semua token yang muncul di data hasil cleaning tahap awal\n",
    "whitelist = set(whitelist)\n",
    "slang_keys = set(slang.keys())\n",
    "slang_values = set(slang.values())\n",
    "typo_keys = set(typo.keys())\n",
    "emoji_mapping = set(emoji.values())\n",
    "laughter_keys = set(laughter)\n",
    "affix_keys = set(affix_map.keys())\n",
    "\n",
    "covered = set()\n",
    "\n",
    "covered |= whitelist\n",
    "covered |= slang_keys\n",
    "covered |= slang_values\n",
    "covered |= typo_keys\n",
    "covered |= emoji_mapping\n",
    "covered |= affix_keys\n",
    "covered |= laughter_keys\n",
    "covered |= affix_keys\n",
    "\n",
    "uncovered_tokens = tokens_interim_set - covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5938096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freq = Counter(tokens_clean)\n",
    "uncovered_freq = {tok: freq[tok] for tok in uncovered_tokens}\n",
    "uncovered_sorted = sorted(uncovered_freq.items(), key=lambda x: -x[1])\n",
    "filtered = [(tok, freq) for tok, freq in uncovered_sorted if not tok.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "218e0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"token\": tok, \"count\": freq} for tok, freq in filtered]\n",
    "\n",
    "with open(\"uncovered_sorted.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b382d",
   "metadata": {},
   "source": [
    "# üìù Example Raw Review  \n",
    "\n",
    "Let‚Äôs inspect the most noisy raw review to understand the noise present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84221938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPLAIN CLEANING PIPELINE ===\n",
      "Input: \n",
      "WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\n",
      "\n",
      "---------------------------------\n",
      "[Lowercase]\n",
      "  before: \n",
      "WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\n",
      "\n",
      "  after : \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: testuser@gmail.com,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\n",
      "\n",
      "---------------------------------\n",
      "[Remove Links]\n",
      "  before: \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: testuser@gmail.com,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\n",
      "\n",
      "  after : \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh:   gk tauuu kenapaaa, email-ku:  ,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\n",
      "\n",
      "---------------------------------\n",
      "[Remove Punctuation]\n",
      "  before: \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh:   gk tauuu kenapaaa, email-ku:  ,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\n",
      "\n",
      "  after : \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp     lmoott bnaget    sumpaaahhh üò°üò°\n",
      "cek ini deh    gk tauuu kenapaaa  email ku      \n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg    \n",
      "\n",
      "---------------------------------\n",
      "[Normalize Laughter]\n",
      "  before: \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp     lmoott bnaget    sumpaaahhh üò°üò°\n",
      "cek ini deh    gk tauuu kenapaaa  email ku      \n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg    \n",
      "\n",
      "  after : wkwkwkwkwk üò≠üò≠üò≠gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh üò°üò°cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠sm tolongggg bgt dongggg\n",
      "---------------------------------\n",
      "[Map Emoji]\n",
      "  before: wkwkwkwkwk üò≠üò≠üò≠gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh üò°üò°cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠sm tolongggg bgt dongggg\n",
      "  after : wkwkwkwkwk  [EMOJI_CRY]  [EMOJI_CRY]  [EMOJI_CRY] gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh  [EMOJI_VERY_ANGRY]  [EMOJI_VERY_ANGRY] cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa  [EMOJI_CRY]  [EMOJI_CRY] sm tolongggg bgt dongggg\n",
      "---------------------------------\n",
      "[Normalize Stretch]\n",
      "  before: wkwkwkwkwk  [EMOJI_CRY]  [EMOJI_CRY]  [EMOJI_CRY] gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh  [EMOJI_VERY_ANGRY]  [EMOJI_VERY_ANGRY] cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa  [EMOJI_CRY]  [EMOJI_CRY] sm tolongggg bgt dongggg\n",
      "  after : wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] gk bisaa login skrg pls help lmot bnaget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sm tolong bgt dongg\n",
      "---------------------------------\n",
      "[Normalize Typos]\n",
      "  before: wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] gk bisaa login skrg pls help lmot bnaget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sm tolong bgt dongg\n",
      "  after : wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] gk bisaa login sekarang pls help lmot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dongg\n",
      "---------------------------------\n",
      "[Normalize Slang]\n",
      "  before: wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] gk bisaa login sekarang pls help lmot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dongg\n",
      "  after : hahaha [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lmot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dong\n",
      "---------------------------------\n",
      "[Remove Stopwords]\n",
      "  before: hahaha [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lmot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dong\n",
      "  after : hahaha [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lmot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] tolong banget\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hahaha [EMOJI_CRY] [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lmot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] tolong banget'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example =\"\"\"\n",
    "WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
    "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,,\n",
    "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!!\n",
    "\"\"\"\n",
    "\n",
    "cleaner.explain(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3a4a8",
   "metadata": {},
   "source": [
    "# üöÄ Applying the Full Cleaning Pipeline\n",
    "\n",
    "Now that each cleaning step has been validated individually,\n",
    "we apply the full `clean_text()` function to the entire dataset.\n",
    "\n",
    "This ensures all reviews follow a standardized, noise-free text format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

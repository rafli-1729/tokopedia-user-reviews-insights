{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee481b7",
   "metadata": {},
   "source": [
    "# üßº **Text Cleaning for Tokopedia User Reviews**  \n",
    "This notebook performs systematic text cleaning on raw, user-generated reviews collected from the Tokopedia application.\n",
    "\n",
    "User reviews typically contain substantial noise, such as:\n",
    "\n",
    "- emojis and unicode icons\n",
    "- URLs and emails\n",
    "- excessive character repetitions (‚Äúbaaaagus bangeeetttt‚Äù)\n",
    "- exaggerated laughter (‚Äúwkwkwkwkwk‚Äù, ‚Äúhahahahaha‚Äù)\n",
    "- slang_map and informal spellings (‚Äúgk‚Äù, ‚Äúga‚Äù, ‚Äúbgt‚Äù, ‚Äúplis‚Äù)\n",
    "- typos and phonetic spelling\n",
    "- punctuation noise\n",
    "- extremely short or low-information messages (‚Äúok‚Äù, ‚Äú.‚Äù)\n",
    "\n",
    "Cleaning these reviews is essential to:\n",
    "\n",
    "- reduce vocabulary sparsity  \n",
    "- standardize spelling variations  \n",
    "- improve downstream NLP model quality  \n",
    "- remove meaningless tokens  \n",
    "- prepare the text for vectorization and modeling  \n",
    "\n",
    "This notebook runs through the process **step-by-step**, showing before/after transformations to highlight the effect of each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6e9f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Directory alignment and module update\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Ignore warning\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Core library\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Cleaning tools\n",
    "import src.cleaning as cleaning\n",
    "\n",
    "# Reload shortcut\n",
    "def r(module=cleaning):\n",
    "    importlib.reload(module)\n",
    "\n",
    "RAW_PATH = '../data/raw/review.csv'\n",
    "\n",
    "# Defaults\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643a677",
   "metadata": {},
   "source": [
    "# üîç **Load Raw Review Data**  \n",
    "\n",
    "We start by loading the unprocessed user reviews from the dataset. Only the raw text column will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31dfa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>char_len</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 11:01:10</td>\n",
       "      <td>87.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Memuaskan kan produk original</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:35:41</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 10:11:21</td>\n",
       "      <td>124.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jos mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:04:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 09:54:44</td>\n",
       "      <td>66.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "0                                       belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai   \n",
       "1                                                                                                 Memuaskan kan produk original   \n",
       "2  mau nyari apa aja di mesin pencariannya TOKOPEDIA hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya   \n",
       "3                                                                                                                    jos mantap   \n",
       "4                                                            Tidak punya CS hanya ada bot yg tidak bisa memberikan solusi BURUK   \n",
       "\n",
       "   rating                 date  char_len  token_len  \n",
       "0       5  2025-12-03 11:01:10      87.0         13  \n",
       "1       5  2025-12-03 10:35:41      29.0          4  \n",
       "2       1  2025-12-03 10:11:21     124.0         20  \n",
       "3       5  2025-12-03 10:04:00      10.0          2  \n",
       "4       1  2025-12-03 09:54:44      66.0         12  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868cda1",
   "metadata": {},
   "source": [
    "# üìö **Loading Assets**\n",
    "\n",
    "- `emoji_map.json`  \n",
    "  A mapping from emoji characters to alphabetic tokens. Emojis often introduce encoding issues and increase vocabulary size. By converting them into readable alphabetic placeholders, downstream models can process text more reliably and consistently.\n",
    "\n",
    "- `pos_lexicon.json` \n",
    "  A part-of-speech lexicon used for advanced linguistic processing. This asset helps capture semantic structure (verbs, nouns, adjectives, etc.), allowing more accurate downstream interpretation such as dependency contexts or rule-based transformations.\n",
    "\n",
    "- `slang_map.json`\n",
    "  Contains mappings from Indonesian slang terms to their canonical (formal) forms. Since slang appears heavily in user-generated content, normalizing them reduces vocabulary explosion and improves model robustness.\n",
    "\n",
    "- `typo_map.json`\n",
    "  A list of common typos mapped directly to their canonical forms. Typos that originate from slang (e.g., \"gppp\" ‚Üí \"gapapa\") are resolved directly to the final standard word to avoid redundant two-step normalization (typo ‚Üí slang ‚Üí canonical).\n",
    "\n",
    "- `affix_map.json`  \n",
    "  A curated dictionary of valid stems and affixed words mined from the dataset. After performing automated stemming (Sastrawi), we cross-check the results with the whitelist to identify legitimate forms. This map prevents incorrect word-splitting during normalization (e.g., \"dibukakan\" shouldn't be split into \"di\" + \"bukak\" + \"an\").\n",
    "\n",
    "- `stopwords.txt`\n",
    "  A list of low-information words that can be ignored during analysis. Useful for reducing noise in tasks such as topic modeling or weighting in classical NLP.\n",
    "\n",
    "- `whitelist.txt`\n",
    "  A list of valid base words derived from KBBI and curated dataset vocabulary. Used to validate stems, prevent over-stemming, and guide morphological rules in the cleaning pipeline.\n",
    "\n",
    "- `laughter.txt`\n",
    "  A collection of Indonesian laughter variants (e.g., \"wkwk\", \"awkawk\", \"xixixi\", \"hehe\", \"kekeke\"). Since laughter expressions are extremely diverse and model-breaking, this list ensures consistent normalization to a stable token.\n",
    "\n",
    "- `negation.txt`\n",
    "  Contains Indonesian negation words (e.g., \"tidak\", \"tak\", \"nggak\", \"bukan\"). Important for future tasks such as sentiment analysis, where negation flipping plays a significant semantic role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d443ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/emoji_map.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    emoji_map = json.load(f)\n",
    "\n",
    "with open(\"../assets/pos_lexicon.json\") as f:\n",
    "    pos_lexicon = json.load(f)\n",
    "\n",
    "with open(\"../assets/slang_map.json\") as f:\n",
    "    slang_map = json.load(f)\n",
    "\n",
    "with open(\"../assets/typo_map.json\") as f:\n",
    "    typo_map = json.load(f)\n",
    "\n",
    "with open(\"../assets/affix_map.json\") as f:\n",
    "    affix_map = json.load(f)\n",
    "\n",
    "with open(\"../assets/stopwords.txt\") as f:\n",
    "    stopwords = [x.strip() for x in f]\n",
    "\n",
    "with open(\"../assets/whitelist.txt\") as f:\n",
    "    whitelist = [x.strip() for x in f]\n",
    "\n",
    "with open(\"../assets/laughter.txt\") as f:\n",
    "    laughter = [x.strip() for x in f]\n",
    "\n",
    "with open(\"../assets/negation.txt\") as f:\n",
    "    negation = [x.strip() for x in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca22d3c",
   "metadata": {},
   "source": [
    "# üìù Example Raw Review  \n",
    "\n",
    "To understand the types of noise present in the dataset, we inspect one of the most unclean raw reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea515f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPLAIN CLEANING PIPELINE ===\n",
      "Input: \n",
      "WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!! HARI2 GINI MuLu 2JIRR 3KOCAKK2\n",
      "\n",
      "---------------------------------\n",
      "[Lowercase]\n",
      "  before: \n",
      "WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!! HARI2 GINI MuLu 2JIRR 3KOCAKK2\n",
      "\n",
      "  after : \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: testuser@gmail.com,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!! hari2 gini mulu 2jirr 3kocakk2\n",
      "\n",
      "---------------------------------\n",
      "[Remove Links]\n",
      "  before: \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: testuser@gmail.com,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!! hari2 gini mulu 2jirr 3kocakk2\n",
      "\n",
      "  after : \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh:   gk tauuu kenapaaa, email-ku:  ,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!! hari2 gini mulu 2jirr 3kocakk2\n",
      "\n",
      "---------------------------------\n",
      "[Remove Punctuation]\n",
      "  before: \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
      "cek ini deh:   gk tauuu kenapaaa, email-ku:  ,,,\n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!! hari2 gini mulu 2jirr 3kocakk2\n",
      "\n",
      "  after : \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp     lmoott bnaget    sumpaaahhh üò°üò°\n",
      "cek ini deh    gk tauuu kenapaaa  email ku      \n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg     hari2 gini mulu 2jirr 3kocakk2\n",
      "\n",
      "---------------------------------\n",
      "[Handle Word Number]\n",
      "  before: \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp     lmoott bnaget    sumpaaahhh üò°üò°\n",
      "cek ini deh    gk tauuu kenapaaa  email ku      \n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg     hari2 gini mulu 2jirr 3kocakk2\n",
      "\n",
      "  after : \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp     lmoott bnaget    sumpaaahhh üò°üò°\n",
      "cek ini deh    gk tauuu kenapaaa  email ku      \n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg     hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "\n",
      "---------------------------------\n",
      "[Normalize Laughter]\n",
      "  before: \n",
      "wkwkwkwkwküò≠üò≠üò≠ gk bisaaa login skrggg plsssss helpppp     lmoott bnaget    sumpaaahhh üò°üò°\n",
      "cek ini deh    gk tauuu kenapaaa  email ku      \n",
      "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg     hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "\n",
      "  after : wkwkwkwkwk üò≠üò≠üò≠gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh üò°üò°cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠sm tolongggg bgt dongggg hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "---------------------------------\n",
      "[Map Emoji]\n",
      "  before: wkwkwkwkwk üò≠üò≠üò≠gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh üò°üò°cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠sm tolongggg bgt dongggg hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "  after : wkwkwkwkwk  [EMOJI_CRY]  [EMOJI_CRY]  [EMOJI_CRY] gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh  [EMOJI_VERY_ANGRY]  [EMOJI_VERY_ANGRY] cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa  [EMOJI_CRY]  [EMOJI_CRY] sm tolongggg bgt dongggg hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "---------------------------------\n",
      "[Collapse Emoji Placeholders]\n",
      "  before: wkwkwkwkwk  [EMOJI_CRY]  [EMOJI_CRY]  [EMOJI_CRY] gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh  [EMOJI_VERY_ANGRY]  [EMOJI_VERY_ANGRY] cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa  [EMOJI_CRY]  [EMOJI_CRY] sm tolongggg bgt dongggg hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "  after : wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa [EMOJI_CRY] [EMOJI_CRY] sm tolongggg bgt dongggg hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "---------------------------------\n",
      "[Normalize Stretch]\n",
      "  before: wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] gk bisaaa login skrggg plsssss helpppp lmoott bnaget sumpaaahhh [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauuu kenapaaa email ku lamaaaaaa bangettttt prosesnyyyyaaaa [EMOJI_CRY] [EMOJI_CRY] sm tolongggg bgt dongggg hari hari gini mulu 2 jirr 3 kocakk kocakk\n",
      "  after : wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] gk bisaa login skrg pls help lmot bnaget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sm tolong bgt dongg hari hari gini mulu 2 jir 3 kocak kocak\n",
      "---------------------------------\n",
      "[Normalize Typos]\n",
      "  before: wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] gk bisaa login skrg pls help lmot bnaget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sm tolong bgt dongg hari hari gini mulu 2 jir 3 kocak kocak\n",
      "  after : wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] gk bisaa login sekarang pls help lemot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dongg hari hari gini mulu 2 jir 3 kocak kocak\n",
      "---------------------------------\n",
      "[Normalize slang_map]\n",
      "  before: wkwkwkwkwk [EMOJI_CRY] [EMOJI_CRY] gk bisaa login sekarang pls help lemot banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh gk tauu kenapaa email ku lama bangett prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dongg hari hari gini mulu 2 jir 3 kocak kocak\n",
      "  after : hahaha [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lambat banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dong hari hari begini melulu 2 anjing 3 kocak kocak\n",
      "---------------------------------\n",
      "[Remove Stopwords]\n",
      "  before: hahaha [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lambat banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek ini deh tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] sama tolong banget dong hari hari begini melulu 2 anjing 3 kocak kocak\n",
      "  after : hahaha [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lambat banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] tolong banget hari hari begini melulu 2 anjing 3 kocak kocak\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hahaha [EMOJI_CRY] [EMOJI_CRY] tidak bisa masuk sekarang mohon help lambat banget sumpah [EMOJI_VERY_ANGRY] [EMOJI_VERY_ANGRY] cek tidak tahu kenapa email ku lama sangat prosesnya [EMOJI_CRY] [EMOJI_CRY] tolong banget hari hari begini melulu 2 anjing 3 kocak kocak'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r(cleaning)\n",
    "\n",
    "cleaner = cleaning.CleaningPipeline(\n",
    "    whitelist=whitelist, slang_map=slang_map, typo_map=typo_map,\n",
    "    emoji_map=emoji_map, laughter_list=laughter, stopwords=stopwords,\n",
    "    pos_lexicon=pos_lexicon, negation_list=negation, affix_map=affix_map\n",
    ")\n",
    "\n",
    "example =\"\"\"\n",
    "WKWKWKWKWKüò≠üò≠üò≠ gk bisaaa login SKRGGG plsssss helpppp!!!! lmoott bnaget... sumpaaahhh üò°üò°\n",
    "cek ini deh: https://tokopedia.com/login-error gk tauuu kenapaaa, email-ku: TESTUSER@GMAIL.COM,,,\n",
    "lamaaaaaa bangettttt prosesnyyyyaaaa üò≠üò≠ sm tolongggg bgt dongggg!!!! HARI2 GINI MuLu 2JIRR 3KOCAKK2\n",
    "\"\"\"\n",
    "\n",
    "cleaner.explain(example, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ec60e",
   "metadata": {},
   "source": [
    "The `CleaningPipeline` applies a sequence of transformations to each review: it first normalizes Unicode and lowercases the text, then removes emails/URLs and punctuation while handling word‚Äìnumber patterns. Next, it normalizes laughter variants and maps emojis to text tokens. After that, it collapses character stretching, splits attached compound words, and fixes common typos before mapping slang terms to their canonical forms. Finally, it removes stopwords, normalizes whitespace, and drops low-information texts, resulting in a clean, standardized corpus ready for downstream modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e54b5a",
   "metadata": {},
   "source": [
    "# üí° **Baseline Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94c27f",
   "metadata": {},
   "source": [
    "Natural language processing especially from scraped data can be very noisy e.g. random unicode, different capitalize format, emojis, random link, and spams. Those noise should be eliminated to make efficient model. Thus we'll do some basic cleaning ensuring all of those possible noise removed properly.\n",
    "\n",
    "**Note:** To make this process more efficient, we'll make a dictionary of cached words that has been normalized before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b72bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4030f780e7cf42268eeba91d81f4bdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STRETCH_CACHE = {}\n",
    "LAUGHTER_CACHE = {}\n",
    "\n",
    "def cached_stretch(word):\n",
    "    w = word.lower()\n",
    "\n",
    "    if w in STRETCH_CACHE:\n",
    "        return STRETCH_CACHE[w]\n",
    "\n",
    "    result = cleaner._stretch_all(w)\n",
    "\n",
    "    STRETCH_CACHE[w] = result\n",
    "    return result\n",
    "\n",
    "def cached_laughter(word):\n",
    "    w = word.lower()\n",
    "\n",
    "    if w in LAUGHTER_CACHE:\n",
    "        return LAUGHTER_CACHE[w]\n",
    "\n",
    "    result = cleaner._normalize_laughter(w)\n",
    "\n",
    "    LAUGHTER_CACHE[w] = result\n",
    "    return result\n",
    "\n",
    "# Baseline cleaning\n",
    "def baseline_cleaning(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = cleaner._normalize_unicode(text)\n",
    "    text = text.lower()\n",
    "    text = cleaner._remove_email_and_link(text)\n",
    "    text = cleaner._remove_punctuation(text)\n",
    "    text = cleaner._handle_word_number(text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [\n",
    "        cached_stretch(tok)\n",
    "        for tok in tokens\n",
    "    ]\n",
    "    tokens = [\n",
    "        cached_laughter(tok)\n",
    "        for tok in tokens\n",
    "    ]\n",
    "    text = \" \".join(tokens)\n",
    "    text = cleaner._map_emoji(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df[\"baseline_text\"] = df[\"text\"].progress_apply(\n",
    "    lambda sentence: baseline_cleaning(sentence)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0a318",
   "metadata": {},
   "source": [
    "Since the amount of tokens and length of text is changed, we'll replace them with current text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bccfb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline cleaning save\n",
    "df_baseline = df[['baseline_text', 'rating', 'date']]\n",
    "df_baseline.columns = ['text', 'rating', 'date']\n",
    "\n",
    "# Additional features\n",
    "df_baseline[\"char_len\"] = df_baseline[\"text\"].str.len()\n",
    "df_baseline[\"token_len\"] = (\n",
    "    df_baseline[\"text\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.split()\n",
    "    .apply(len)\n",
    ")\n",
    "\n",
    "df_baseline['text'] = df_baseline['text'].astype(str).str.strip()\n",
    "df_baseline = df_baseline[df_baseline['text'] != \"\"]\n",
    "\n",
    "# Save to csv and txt\n",
    "df_baseline.to_csv('../data/baseline/review.csv', index=False)\n",
    "with open(\"../data/baseline/all_reviews.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df_baseline.text.astype(str):\n",
    "        f.write(line.replace(\"\\n\", \" \") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42068bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>char_len</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 11:01:10</td>\n",
       "      <td>87</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memuaskan kan produk original</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:35:41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mau nyari apa aja di mesin pencarianya tokopedia hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 10:11:21</td>\n",
       "      <td>123</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jos mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:04:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tidak punya cs hanya ada bot yg tidak bisa memberikan solusi buruk</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 09:54:44</td>\n",
       "      <td>66</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                          text  \\\n",
       "0                                      belanja di tokopedia sangat mudah cuma sayang nya estimasi pengiriman yang tidak sesuai   \n",
       "1                                                                                                memuaskan kan produk original   \n",
       "2  mau nyari apa aja di mesin pencarianya tokopedia hasil timeout melulu padahal sinyal bagus maen game online aja lancar jaya   \n",
       "3                                                                                                                   jos mantap   \n",
       "4                                                           tidak punya cs hanya ada bot yg tidak bisa memberikan solusi buruk   \n",
       "\n",
       "   rating                 date  char_len  token_len  \n",
       "0       5  2025-12-03 11:01:10        87         13  \n",
       "1       5  2025-12-03 10:35:41        29          4  \n",
       "2       1  2025-12-03 10:11:21       123         20  \n",
       "3       5  2025-12-03 10:04:00        10          2  \n",
       "4       1  2025-12-03 09:54:44        66         12  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709744f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>60</th>\n",
       "      <th>134</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>raw</th>\n",
       "      <td>malas belanja disini, gak dpt gratis ongkir, diakun sy gak dpt promo murah, mahal, saya keluarüôèüôèmaaf</td>\n",
       "      <td>payah susah suka ngelek............................. ......................................................................................</td>\n",
       "      <td>Mohon maaf, akun saya tiba-tiba dibatalkan saat akan melakukan check-out, alhasil, saya tidak diberi akses promosi Tokopedia, Voucher eXtra, dan Tokopedia Plus. Ini sangat merugikan, sedangkan penanganan CS tidak jelas kapan diperbaiki.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>malas belanja disini gak dpt gratis ongkir diakun sy gak dpt promo murah mahal saya keluar  [EMOJI_PRAY] maf</td>\n",
       "      <td>payah susah suka ngelek</td>\n",
       "      <td>mohon maaf akun saya tiba tiba dibatalkan saat akan melakukan check out alhasil saya tidak diberi akses promosi tokopedia voucher extra dan tokopedia plus ini sangat merugikan sedangkan penanganan cs tidak jelas kapan diperbaiki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   10   \\\n",
       "raw               malas belanja disini, gak dpt gratis ongkir, diakun sy gak dpt promo murah, mahal, saya keluarüôèüôèmaaf   \n",
       "baseline  malas belanja disini gak dpt gratis ongkir diakun sy gak dpt promo murah mahal saya keluar  [EMOJI_PRAY] maf   \n",
       "\n",
       "                                                                                                                                                  60   \\\n",
       "raw       payah susah suka ngelek............................. ......................................................................................   \n",
       "baseline                                                                                                                      payah susah suka ngelek   \n",
       "\n",
       "                                                                                                                                                                                                                                                   134  \n",
       "raw       Mohon maaf, akun saya tiba-tiba dibatalkan saat akan melakukan check-out, alhasil, saya tidak diberi akses promosi Tokopedia, Voucher eXtra, dan Tokopedia Plus. Ini sangat merugikan, sedangkan penanganan CS tidak jelas kapan diperbaiki.  \n",
       "baseline          mohon maaf akun saya tiba tiba dibatalkan saat akan melakukan check out alhasil saya tidak diberi akses promosi tokopedia voucher extra dan tokopedia plus ini sangat merugikan sedangkan penanganan cs tidak jelas kapan diperbaiki  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_compare = [10, 60, 134]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [df['text'].iloc[index_to_compare,],\n",
    "    df_baseline['text'].iloc[index_to_compare,]],\n",
    "    index = ['raw', 'baseline']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1890dec",
   "metadata": {},
   "source": [
    "After baseline cleaning above, we can immediately notice that all the noise possible is already mapped such as case typing, punctuation, emoji is mapped and any other possible noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87741613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['baseline_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e858d",
   "metadata": {},
   "source": [
    "# üöÄ **Fully Cleaned Dataset**\n",
    "\n",
    "Now that each cleaning step has been validated, we apply the full `CleaningPipeline.explain()` function to the entire dataset. This produces a fully standardized and noise-reduced text corpus that is ready for tokenization and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b2ab567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b451b7eaab9944c6a63550d9f1efb3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].progress_apply(\n",
    "    lambda sentence: cleaner.explain(sentence, verbose=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a88c2",
   "metadata": {},
   "source": [
    "Since the amount of tokens and length of text is changed, we'll replace them with current text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33757ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Cleaning\n",
    "df_clean = df[['clean_text', 'rating', 'date']]\n",
    "df_clean.columns = ['text', 'rating', 'date']\n",
    "\n",
    "# Additional features\n",
    "df_clean[\"char_len\"] = df_clean[\"text\"].str.len()\n",
    "df_clean[\"token_len\"] = (\n",
    "    df_clean[\"text\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.split()\n",
    "    .apply(len)\n",
    ")\n",
    "\n",
    "df_clean['text'] = df_clean['text'].astype(str).str.strip()\n",
    "df_clean = df_clean[df_clean['text'] != \"\"]\n",
    "\n",
    "df_clean.to_csv('../data/clean/review.csv', index=False)\n",
    "with open(\"../data/clean/all_reviews.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df_clean.text.astype(str):\n",
    "        f.write(line.replace(\"\\n\", \" \") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c2ee70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>char_len</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>belanja sangat mudah sayang estimasi pengiriman tidak sesuai</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 11:01:10</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memuaskan produk original</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:35:41</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mencari mesin pencarianya hasil timeout melulu padahal jaringan bagus main game online lancar jaya</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 10:11:21</td>\n",
       "      <td>98</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jos mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-03 10:04:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tidak customer service bot tidak bisa memberikan solusi buruk</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-03 09:54:44</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "0                                        belanja sangat mudah sayang estimasi pengiriman tidak sesuai   \n",
       "1                                                                           memuaskan produk original   \n",
       "2  mencari mesin pencarianya hasil timeout melulu padahal jaringan bagus main game online lancar jaya   \n",
       "3                                                                                          jos mantap   \n",
       "4                                       tidak customer service bot tidak bisa memberikan solusi buruk   \n",
       "\n",
       "   rating                 date  char_len  token_len  \n",
       "0       5  2025-12-03 11:01:10        60          8  \n",
       "1       5  2025-12-03 10:35:41        25          3  \n",
       "2       1  2025-12-03 10:11:21        98         14  \n",
       "3       5  2025-12-03 10:04:00        10          2  \n",
       "4       1  2025-12-03 09:54:44        61          9  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6cd76",
   "metadata": {},
   "source": [
    "We can immediately tell between raw text and cleaned text. With cleaned text, we reduce the amount of token that'll make our next step more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b063cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['clean_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50c14d",
   "metadata": {},
   "source": [
    "# üñãÔ∏è **Possible Typo Mappings**\n",
    "\n",
    "With our resources above, we can make a list of typo_map hypothetically based on word that did not appear on our whitelist or `slang_map` dictionary. This process can help us to detect any possible `typo_map` and add them into our resources to make dataset even more clean.\n",
    "\n",
    "After Cleaning, we'll do some exploration between the amount of tokens total and unique for each dataset. This step can give us insights that how noisy our dataset actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4496db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw dataset      : (709000, 5)\n",
      "Shape of baseline dataset : (708946, 5)\n",
      "Shape of clean dataset    : (486438, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of raw dataset      :', df.shape)\n",
    "print('Shape of baseline dataset :', df_baseline.shape)\n",
    "print('Shape of clean dataset    :', df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84210856",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_raw = [\n",
    "    word for sentence in df.text.astype(str)\n",
    "    for word in sentence.split()\n",
    "]\n",
    "\n",
    "tokens_baseline = [\n",
    "    word for sentence in df_baseline.text.astype(str)\n",
    "    for word in sentence.split()\n",
    "]\n",
    "\n",
    "tokens_clean = [\n",
    "    word for sentence in df_clean.text.astype(str)\n",
    "    for word in sentence.split()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f87e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tokens length      : 5412151\n",
      "Baseline Tokens Length : 5587881\n",
      "Clean Tokens Length    : 3978209\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Tokens length      :\", len(tokens_raw))\n",
    "print(\"Baseline Tokens Length :\", len(tokens_baseline))\n",
    "print(\"Clean Tokens Length    :\", len(tokens_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9416e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw uniques tokens length      : 303552\n",
      "Baseline uniques tokens length : 77738\n",
      "Clean uniques tokens length    : 59560\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw uniques tokens length      :\", len(set(tokens_raw)))\n",
    "print(\"Baseline uniques tokens length :\", len(set(tokens_baseline)))\n",
    "print(\"Clean uniques tokens length    :\", len(set(tokens_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d31923",
   "metadata": {},
   "source": [
    "Based on unique and total amount of tokens, between raw and baseline can happen because we handle compound word consisting of word and number (e.g. *hari2* -> *hari hari*, *2hari* -> *2 hari*). Splitting them can make the amount of baseline tokens more than raw tokens.\n",
    "\n",
    "On the other side, between baseline and clean tokens is actually expectable because we do other things to make uniques tokens follow one single format such as typo and slang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae16174",
   "metadata": {},
   "outputs": [],
   "source": [
    "covered = set()\n",
    "\n",
    "covered |= set(whitelist)\n",
    "covered |= set(slang_map.keys())\n",
    "covered |= set(slang_map.values())\n",
    "covered |= set(typo_map.values())\n",
    "covered |= set(emoji_map.values())\n",
    "covered |= set(laughter)\n",
    "covered |= set(stopwords)\n",
    "covered |= set(affix_map.keys())\n",
    "\n",
    "uncovered_tokens = set(tokens_baseline) - covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5938096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[EMOJI_MISC]', 28288),\n",
       " ('prakerja', 11700),\n",
       " ('best', 5480),\n",
       " ('service', 5350),\n",
       " ('gopay', 4823)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(tokens_clean)\n",
    "uncovered_freq = {tok: freq[tok] for tok in uncovered_tokens}\n",
    "uncovered_sorted = sorted(uncovered_freq.items(), key=lambda x: -x[1])\n",
    "filtered = [(tok, freq) for tok, freq in uncovered_sorted if not tok.isdigit()]\n",
    "\n",
    "filtered[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "218e0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_inspect = [{\"token\": tok, \"count\": freq} for tok, freq in filtered]\n",
    "\n",
    "with open(\"../assets/to_inspect.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(to_inspect, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a437d91",
   "metadata": {},
   "source": [
    "With to be inspect mapping above, we can actually make our data even more cleaner by adding more word to our assets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f32871",
   "metadata": {},
   "source": [
    "# ü™® **Next Steps**\n",
    "\n",
    "After the data cleaning stage, the next step is to convert the cleaned text into a form that can be used directly for modeling. This includes performing basic exploratory checks to ensure the cleaning pipeline worked as expected such as verifying token distributions, text lengths, and vocabulary size. Once the dataset looks consistent, we proceed with tokenization, either by training a custom tokenizer or using an existing one that fits our domain. The goal is to produce stable token sequences that the model can learn from.\n",
    "\n",
    "After tokenization, we prepare model-ready inputs such as numerical token IDs, attention masks, or TF-IDF vectors depending on the modeling approach. We also finalize the train validation split to ensure balanced evaluation. At this point, the dataset is fully preprocessed and ready to be fed into baseline models or more advanced architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
